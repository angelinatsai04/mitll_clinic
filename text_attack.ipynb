{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f0b450-43a9-4e17-ae46-b064a8f6dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: textattack 0.3.10\n",
      "Uninstalling textattack-0.3.10:\n",
      "  Successfully uninstalled textattack-0.3.10\n",
      "Found existing installation: transformers 4.56.2\n",
      "Uninstalling transformers-4.56.2:\n",
      "  Successfully uninstalled transformers-4.56.2\n",
      "Found existing installation: datasets 2.4.0\n",
      "Uninstalling datasets-2.4.0:\n",
      "  Successfully uninstalled datasets-2.4.0\n",
      "Found existing installation: pyarrow 21.0.0\n",
      "Uninstalling pyarrow-21.0.0:\n",
      "  Successfully uninstalled pyarrow-21.0.0\n",
      "Found existing installation: torch 2.7.1+cu118\n",
      "Uninstalling torch-2.7.1+cu118:\n",
      "  Successfully uninstalled torch-2.7.1+cu118\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (77.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n",
      "Installing collected packages: torch\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bert-score 0.3.13 requires transformers>=3.0.0, which is not installed.\n",
      "transformer-smaller-training-vocab 0.4.2 requires transformers[sentencepiece,torch]<5.0,>=4.1, which is not installed.\n",
      "flair 0.15.1 requires transformers[sentencepiece]<5.0.0,>=4.25.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.7.1+cu118\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting textattack\n",
      "  Using cached textattack-0.3.10-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from textattack) (0.3.13)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from textattack) (0.8.1)\n",
      "Requirement already satisfied: flair in /usr/local/lib/python3.11/dist-packages (from textattack) (0.15.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from textattack) (3.16.1)\n",
      "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.11/dist-packages (from textattack) (2.9.4)\n",
      "Requirement already satisfied: lemminflect in /usr/local/lib/python3.11/dist-packages (from textattack) (0.2.3)\n",
      "Requirement already satisfied: lru-dict in /usr/local/lib/python3.11/dist-packages (from textattack) (1.3.0)\n",
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from textattack) (4.1.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from textattack) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from textattack) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from textattack) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from textattack) (1.15.3)\n",
      "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from textattack) (2.7.1+cu118)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from textattack) (4.56.2)\n",
      "Requirement already satisfied: terminaltables in /usr/local/lib/python3.11/dist-packages (from textattack) (3.1.10)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from textattack) (4.67.1)\n",
      "Requirement already satisfied: word2number in /usr/local/lib/python3.11/dist-packages (from textattack) (1.1)\n",
      "Requirement already satisfied: num2words in /usr/local/lib/python3.11/dist-packages (from textattack) (0.5.14)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from textattack) (10.8.0)\n",
      "Requirement already satisfied: pinyin>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from textattack) (0.4.0)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from textattack) (0.42.1)\n",
      "Requirement already satisfied: OpenHowNet in /usr/local/lib/python3.11/dist-packages (from textattack) (2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score>=0.3.5->textattack) (2.32.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score>=0.3.5->textattack) (3.10.6)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score>=0.3.5->textattack) (24.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.4.0->textattack) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.4.0->textattack) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.4.0->textattack) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.4.0->textattack) (0.70.13)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.4.0->textattack) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.4.0->textattack) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->textattack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->textattack) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->textattack) (2025.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch!=1.8,>=1.7.0->textattack) (77.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->textattack) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->textattack) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->textattack) (0.6.2)\n",
      "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (1.40.44)\n",
      "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (4.5.3)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (1.2.18)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (6.3.1)\n",
      "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (5.2.0)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (1.0.9)\n",
      "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (6.0.2)\n",
      "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (0.5.11)\n",
      "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (3.1)\n",
      "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (0.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (1.3.2)\n",
      "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (1.5.11)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (2.1.0)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (0.9.0)\n",
      "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (0.4.2)\n",
      "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (0.8.1)\n",
      "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair->textattack) (2.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python->textattack) (7.0.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python->textattack) (0.10.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->textattack) (8.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->textattack) (1.5.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from num2words->textattack) (0.6.2)\n",
      "Requirement already satisfied: anytree in /usr/local/lib/python3.11/dist-packages (from OpenHowNet->textattack) (2.13.0)\n",
      "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair->textattack) (4.0.0)\n",
      "Requirement already satisfied: intervaltree in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair->textattack) (3.1.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.44 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair->textattack) (1.40.44)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair->textattack) (0.14.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.17.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (3.12.15)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair->textattack) (4.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.4.0->textattack) (1.1.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from langdetect>=1.0.9->flair->textattack) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score>=0.3.5->textattack) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair->textattack) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair->textattack) (6.32.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (1.20.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (1.10.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.6)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair->textattack) (2.4.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair->textattack) (1.7.1)\n",
      "Using cached textattack-0.3.10-py3-none-any.whl (445 kB)\n",
      "Installing collected packages: textattack\n",
      "Successfully installed textattack-0.3.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall textattack transformers datasets pyarrow torch -y\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pyarrow>=12.0.0\n",
    "!pip install datasets>=2.14.0\n",
    "!pip install transformers>=4.30.0\n",
    "!pip install textattack\n",
    "!pip install scikit-learn scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82477d47-724e-4f89-b70a-7bd498f44907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: datasets 2.19.0\n",
      "Uninstalling datasets-2.19.0:\n",
      "  Successfully uninstalled datasets-2.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Installing collected packages: datasets\n",
      "Successfully installed datasets-4.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall datasets -y\n",
    "!pip install datasets --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861c5538-c963-49c6-91ae-03ad1f7d9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyArrow version: 21.0.0\n",
      "Datasets version: 4.1.1\n",
      "PyTorch version: 2.7.1+cu118\n",
      "Transformers version: 4.56.2\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"PyArrow version: {pa.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6606c43-e7cc-470b-b2b6-636580d04921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "from textattack.attack_recipes import TextFoolerJin2019, PWWSRen2019\n",
    "from textattack import AttackArgs, Attacker\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0620744d-f0d9-4ab0-acf5-8d5d1f48c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Wrapper\n",
    "\n",
    "class AttentionExtractorModel:\n",
    "    def __init__(self, model_name, num_labels=2):\n",
    "        \"\"\"\n",
    "        Initialize model that outputs both predictions and attention weights\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier (e.g., 'bert-base-uncased')\n",
    "            num_labels: Number of classification labels\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load model with output_attentions capability\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,          # classification labels\n",
    "            output_attentions=True # CRITICAL: enables attention extraction\n",
    "        )\n",
    "        \n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def get_attention_maps(self, text):\n",
    "        \"\"\"\n",
    "        Process text and extract attention maps\n",
    "        \n",
    "        Args:\n",
    "            text: Input string\n",
    "            \n",
    "        Returns:\n",
    "            attentions: List of attention tensors [num_layers, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # Tokenize text\n",
    "        encoded = self.tokenizer(\n",
    "            text,                    # str or List[str]\n",
    "            padding=True,            # pad to longest in batch\n",
    "            truncation=True,         # truncate to max_length\n",
    "            max_length=512,          # maximum sequence length\n",
    "            return_tensors='pt'      # return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded)\n",
    "        \n",
    "        # Process attention tensors into numpy arrays\n",
    "        attentions = []\n",
    "        for layer_attention in outputs.attentions:\n",
    "            # layer_attention shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "            attentions.append(layer_attention.cpu().numpy())\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        # Return structured attention data\n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'attentions': attentions,  # List of [batch_size, num_heads, seq_len, seq_len]\n",
    "            'input_ids': encoded['input_ids'].cpu().numpy(),\n",
    "            'attention_mask': encoded['attention_mask'].cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass that returns logits and attention weights\n",
    "        \n",
    "        Returns:\n",
    "            logits: Model predictions\n",
    "            attentions: Tuple of attention tensors from each layer\n",
    "        \"\"\"\n",
    "        # Run model with output_attentions=True\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True  # Must be True to get attentions\n",
    "        )\n",
    "        # Extract and return both logits and attentions\n",
    "        logits = outputs.logits        # [batch_size, num_labels] - predictions\n",
    "        attentions = outputs.attentions    # Tuple of length num_layers\n",
    "\n",
    "        return logits, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983915f1-6c26-405e-b9cc-d33b7cd50b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb86c5ee3ac346eb98480609a71623f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb028ad84944d57931079bb4094621d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ad8d53ce4641c196e9ecf898e032e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654d74e7b1a54e1fb99f982bb6018806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c8519613594097bba3be2be6aa524b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (1, 2)\n",
      "Number of layers: 12\n",
      "Attention shape per layer: (1, 12, 9, 9)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "extractor = AttentionExtractorModel('bert-base-uncased')\n",
    "\n",
    "# Get attention maps\n",
    "text = \"This is a sample text for analysis\"\n",
    "results = extractor.get_attention_maps(text)\n",
    "\n",
    "# Access results\n",
    "print(\"Predictions shape:\", results['predictions'].shape)\n",
    "print(\"Number of layers:\", len(results['attentions']))\n",
    "print(\"Attention shape per layer:\", results['attentions'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f49dd2-b32a-44ee-aab8-ba8bc5435d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc4d44ef-7938-4401-a848-e19b0e5d32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textattack_wrapper\n",
    "\n",
    "class CustomModelWrapper(ModelWrapper):\n",
    "    def __init__(self, model, tokenizer, batch_size=32):\n",
    "        \"\"\"\n",
    "        Wrapper to make your model compatible with TextAttack\n",
    "        \n",
    "        Args:\n",
    "            model: Your PyTorch model\n",
    "            tokenizer: HuggingFace tokenizer\n",
    "            batch_size: Batch size for processing\n",
    "        \"\"\"\n",
    "        # Store model and tokenizer\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # PyTorch tensors need to be on same device to interact\n",
    "        self.device = next(model.parameters()).device\n",
    "    \n",
    "    def __call__(self, text_input_list):\n",
    "        \"\"\"\n",
    "        TextAttack calls this method with a list of strings\n",
    "        \n",
    "        Args:\n",
    "            text_input_list: List of text strings to classify\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Tensor of shape [batch_size, num_labels]\n",
    "        \"\"\"\n",
    "        # Tokenize inputs\n",
    "        encoded = self.tokenizer(\n",
    "            text_input_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to same device as model\n",
    "        encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Get model predictions (logits only, not attentions)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded, output_attentions=False)\n",
    "        \n",
    "        # TextAttack expects raw logits (before softmax)\n",
    "        return outputs.logits\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move model to specified device\"\"\"\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a58f6bc-d9d8-42bd-9a93-87a7893391b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "  0%|          | 0/2 [08:01<?, ?it/s]\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The movie was fantastic and incredibly entertaining!\n",
      "Ground truth label: 1\n",
      "\n",
      "✓ Attack SUCCEEDED!\n",
      "Adversarial text: The movie was grotesque and incredibly harbor!\n",
      "Original prediction: 1\n",
      "Adversarial prediction: 0\n",
      "Number of queries: 51\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack import AttackArgs, Attacker\n",
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "from textattack.attack_recipes import (\n",
    "    BAEGarg2019,           # BERT-based attack\n",
    "    PWWSRen2019,           # Probability Weighted Word Saliency WORKS W/O TENSORFLOW\n",
    "    TextBuggerLi2018,      # Character-level perturbations\n",
    "    DeepWordBugGao2018     # Another character-level attack\n",
    ")\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data first\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load a TRAINED model (fine-tuned on sentiment analysis)\n",
    "model_name = \"textattack/bert-base-uncased-SST-2\"  # Pre-trained on SST-2 sentiment dataset\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create wrapper\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# Create attack\n",
    "attack = PWWSRen2019.build(model_wrapper)\n",
    "\n",
    "# Test on a single example\n",
    "text = \"The movie was fantastic and incredibly entertaining!\"\n",
    "ground_truth_label = 1  # 1 = positive\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Ground truth label: {ground_truth_label}\")\n",
    "\n",
    "# Run attack\n",
    "result = attack.attack(text, ground_truth_label)\n",
    "\n",
    "# Print results properly\n",
    "if isinstance(result, SuccessfulAttackResult):\n",
    "    print(f\"\\n✓ Attack SUCCEEDED!\")\n",
    "    print(f\"Adversarial text: {result.perturbed_text()}\")  # Note: calling the method with ()\n",
    "    print(f\"Original prediction: {result.original_result.output}\")\n",
    "    print(f\"Adversarial prediction: {result.perturbed_result.output}\")\n",
    "    print(f\"Number of queries: {result.num_queries}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Attack FAILED or SKIPPED\")\n",
    "    print(f\"Reason: {type(result).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd7f90-332f-4b35-82a2-d90130a9b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_pipeline\n",
    "\n",
    "def setup_attack(model_wrapper, attack_type='textfooler'):\n",
    "    \"\"\"\n",
    "    Configure TextAttack attack\n",
    "    \n",
    "    Args:\n",
    "        model_wrapper: Your TextAttack-compatible model wrapper\n",
    "        attack_type: Type of attack ('textfooler', 'pwws', etc.)\n",
    "        \n",
    "    Returns:\n",
    "        attacker: Configured Attacker object\n",
    "    \"\"\"\n",
    "    # TODO: Choose attack recipe based on attack_type\n",
    "    # TODO: Build attack with your model wrapper\n",
    "    # TODO: Configure attack arguments (num_examples, etc.)\n",
    "    # TODO: Return configured attacker\n",
    "    pass\n",
    "\n",
    "def generate_adversarial_examples(attacker, dataset, num_examples=100):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples from dataset\n",
    "    \n",
    "    Args:\n",
    "        attacker: TextAttack Attacker object\n",
    "        dataset: Dataset to attack\n",
    "        num_examples: Number of examples to generate\n",
    "        \n",
    "    Returns:\n",
    "        results: List of AttackResult objects\n",
    "    \"\"\"\n",
    "    # TODO: Run attacker on dataset\n",
    "    # TODO: Collect successful and failed attacks\n",
    "    # TODO: Return results with original and perturbed texts\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fd474-c037-4b3a-aba3-f5ecb6358fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_extraction\n",
    "\n",
    "def extract_attention_pairs(model, attack_results):\n",
    "    \"\"\"\n",
    "    Extract attention maps for original and adversarial examples\n",
    "    \n",
    "    Args:\n",
    "        model: Your AttentionExtractorModel\n",
    "        attack_results: Results from TextAttack\n",
    "        \n",
    "    Returns:\n",
    "        attention_data: Dictionary containing paired attention maps\n",
    "    \"\"\"\n",
    "    attention_data = {\n",
    "        'original_texts': [],\n",
    "        'adversarial_texts': [],\n",
    "        'original_attentions': [],  # List of [num_layers, num_heads, seq_len, seq_len]\n",
    "        'adversarial_attentions': [],\n",
    "        'labels': [],\n",
    "        'predictions_original': [],\n",
    "        'predictions_adversarial': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Iterate through attack results\n",
    "    # TODO: For each successful attack:\n",
    "    #       - Extract attention maps from original text\n",
    "    #       - Extract attention maps from adversarial text\n",
    "    #       - Store both with metadata\n",
    "    # TODO: Handle sequence length differences (padding/truncation)\n",
    "    pass\n",
    "\n",
    "def align_attention_maps(attn1, attn2):\n",
    "    \"\"\"\n",
    "    Align attention maps of different sequence lengths\n",
    "    \n",
    "    Args:\n",
    "        attn1, attn2: Attention tensors of potentially different shapes\n",
    "        \n",
    "    Returns:\n",
    "        aligned_attn1, aligned_attn2: Aligned attention maps\n",
    "    \"\"\"\n",
    "    # TODO: Determine shorter sequence length\n",
    "    # TODO: Truncate or pad attention maps to match\n",
    "    # TODO: Handle special tokens ([CLS], [SEP], [PAD])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87b32c-653d-4223-9fa2-dd0ff41e398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "def visualize_attention_head(attention_matrix, tokens, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize a single attention head\n",
    "    \n",
    "    Args:\n",
    "        attention_matrix: 2D attention weights [seq_len, seq_len]\n",
    "        tokens: List of token strings\n",
    "        save_path: Optional path to save figure\n",
    "    \"\"\"\n",
    "    # TODO: Create heatmap with seaborn\n",
    "    # TODO: Add token labels on axes\n",
    "    # TODO: Add colorbar\n",
    "    # TODO: Save or show figure\n",
    "    pass\n",
    "\n",
    "def visualize_layer_comparison(original_attn, adversarial_attn, layer_idx, head_idx):\n",
    "    \"\"\"\n",
    "    Side-by-side comparison of attention maps\n",
    "    \n",
    "    Args:\n",
    "        original_attn: Original attention tensor\n",
    "        adversarial_attn: Adversarial attention tensor\n",
    "        layer_idx: Which layer to visualize\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # TODO: Create 1x2 subplot\n",
    "    # TODO: Plot original attention on left\n",
    "    # TODO: Plot adversarial attention on right\n",
    "    # TODO: Use same color scale for both\n",
    "    pass\n",
    "\n",
    "def plot_attention_difference_map(original_attn, adversarial_attn):\n",
    "    \"\"\"\n",
    "    Visualize the difference between attention patterns\n",
    "    \n",
    "    Args:\n",
    "        original_attn: Original attention [seq_len, seq_len]\n",
    "        adversarial_attn: Adversarial attention [seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # TODO: Compute difference: adversarial - original\n",
    "    # TODO: Plot difference heatmap\n",
    "    # TODO: Use diverging colormap (e.g., RdBu)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3ac47-02a2-4b8a-b2a5-a8a299bf58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topological_analysis\n",
    "\n",
    "def compute_attention_entropy(attention_matrix):\n",
    "    \"\"\"\n",
    "    Compute entropy of attention distribution for each query token\n",
    "    \n",
    "    Args:\n",
    "        attention_matrix: [seq_len, seq_len] attention weights\n",
    "        \n",
    "    Returns:\n",
    "        entropies: Entropy values for each query position\n",
    "    \"\"\"\n",
    "    # TODO: For each row (query token)\n",
    "    # TODO: Compute Shannon entropy: -sum(p * log(p))\n",
    "    # TODO: Return entropy values\n",
    "    pass\n",
    "\n",
    "def compute_attention_distance_metrics(original_attn, adversarial_attn):\n",
    "    \"\"\"\n",
    "    Compute various distance metrics between attention patterns\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary of distance metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # TODO: Flatten attention matrices\n",
    "    # TODO: Compute L2 distance (Euclidean)\n",
    "    # TODO: Compute cosine similarity\n",
    "    # TODO: Compute KL divergence (treat as probability distributions)\n",
    "    # TODO: Compute Wasserstein distance (Earth Mover's Distance)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_attention_graph_metrics(attention_matrix, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Treat attention as a graph and compute graph properties\n",
    "    \n",
    "    Args:\n",
    "        attention_matrix: [seq_len, seq_len]\n",
    "        threshold: Minimum attention weight to consider an edge\n",
    "        \n",
    "    Returns:\n",
    "        graph_metrics: Dictionary of graph properties\n",
    "    \"\"\"\n",
    "    # TODO: Threshold attention to create adjacency matrix\n",
    "    # TODO: Compute degree centrality\n",
    "    # TODO: Compute clustering coefficient\n",
    "    # TODO: Compute connected components\n",
    "    # TODO: Return metrics\n",
    "    pass\n",
    "\n",
    "def analyze_layer_wise_changes(original_attentions, adversarial_attentions):\n",
    "    \"\"\"\n",
    "    Analyze how perturbations propagate through layers\n",
    "    \n",
    "    Args:\n",
    "        original_attentions: List of attention tensors per layer\n",
    "        adversarial_attentions: List of attention tensors per layer\n",
    "        \n",
    "    Returns:\n",
    "        layer_metrics: Per-layer analysis\n",
    "    \"\"\"\n",
    "    layer_metrics = []\n",
    "    \n",
    "    # TODO: For each layer:\n",
    "    #       - Average attention across heads\n",
    "    #       - Compute distance metrics\n",
    "    #       - Compute entropy changes\n",
    "    #       - Track how perturbation effects evolve\n",
    "    \n",
    "    return layer_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a3ed1-c65e-4cb3-96a0-88bda90cd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical_analysis\n",
    "\n",
    "def aggregate_results(attention_data_list):\n",
    "    \"\"\"\n",
    "    Aggregate metrics across multiple adversarial examples\n",
    "    \n",
    "    Args:\n",
    "        attention_data_list: List of attention data dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        aggregated_metrics: Statistical summary\n",
    "    \"\"\"\n",
    "    # TODO: Collect all distance metrics\n",
    "    # TODO: Compute mean, std, median for each metric\n",
    "    # TODO: Perform statistical tests (e.g., t-test, Wilcoxon)\n",
    "    # TODO: Analyze layer-wise trends\n",
    "    pass\n",
    "\n",
    "def analyze_perturbation_impact():\n",
    "    \"\"\"\n",
    "    Analyze which types of perturbations cause largest attention changes\n",
    "    \"\"\"\n",
    "    # TODO: Group by perturbation type (word substitution, insertion, deletion)\n",
    "    # TODO: Compute average attention distance per perturbation type\n",
    "    # TODO: Correlate perturbation position with attention change magnitude\n",
    "    pass\n",
    "\n",
    "def identify_vulnerable_layers():\n",
    "    \"\"\"\n",
    "    Determine which layers are most affected by adversarial perturbations\n",
    "    \"\"\"\n",
    "    # TODO: Compute per-layer attention distance metrics\n",
    "    # TODO: Rank layers by average change magnitude\n",
    "    # TODO: Visualize layer vulnerability\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b95a4-ba50-4f08-b5cb-b1660d1046b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "\n",
    "def main():\n",
    "    # 1. Load model and setup\n",
    "    print(\"Loading model...\")\n",
    "    # TODO: Initialize AttentionExtractorModel\n",
    "    # TODO: Create TextAttack wrapper\n",
    "    \n",
    "    # 2. Setup attack\n",
    "    print(\"Setting up attack...\")\n",
    "    # TODO: Configure attack (TextFooler, PWWS, etc.)\n",
    "    # TODO: Load dataset\n",
    "    \n",
    "    # 3. Generate adversarial examples\n",
    "    print(\"Generating adversarial examples...\")\n",
    "    # TODO: Run attack on dataset\n",
    "    # TODO: Filter successful attacks\n",
    "    \n",
    "    # 4. Extract attention maps\n",
    "    print(\"Extracting attention maps...\")\n",
    "    # TODO: For each original/adversarial pair, extract attentions\n",
    "    # TODO: Save raw attention data\n",
    "    \n",
    "    # 5. Compute topological metrics\n",
    "    print(\"Computing topological metrics...\")\n",
    "    # TODO: Run topological analysis\n",
    "    # TODO: Compute distance metrics\n",
    "    # TODO: Analyze layer-wise changes\n",
    "    \n",
    "    # 6. Visualize results\n",
    "    print(\"Generating visualizations...\")\n",
    "    # TODO: Create attention heatmaps\n",
    "    # TODO: Plot difference maps\n",
    "    # TODO: Create layer-wise comparison plots\n",
    "    \n",
    "    # 7. Statistical analysis\n",
    "    print(\"Performing statistical analysis...\")\n",
    "    # TODO: Aggregate results\n",
    "    # TODO: Test for significance\n",
    "    # TODO: Generate summary report\n",
    "    \n",
    "    # 8. Save results\n",
    "    print(\"Saving results...\")\n",
    "    # TODO: Save processed data\n",
    "    # TODO: Save figures\n",
    "    # TODO: Generate report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textattack-venv)",
   "language": "python",
   "name": "textattack-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
