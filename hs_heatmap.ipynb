{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642bd9ef-18ed-48dd-b482-c833a96ccfae",
   "metadata": {},
   "source": [
    "# Code to make heatmaps using latent representations of data in LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e4af7a-84b5-4c18-af54-a7f3bb53778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klopez/tda_vs_linguistics/mitll2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for attention maps + hidden states\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ripser import ripser\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for nlp\n",
    "import spacy\n",
    "import textdescriptives as td\n",
    "\n",
    "# for heatmaps\n",
    "from dcor import distance_correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import dcor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ca98ed-5c26-4215-ac8e-8c5f5844114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should first do the following in terminal for llama:\n",
    "# 1. pip install huggingface_hub\n",
    "# 2. huggingface-cli login --token <access_token>\n",
    "# alt 2. hf auth login\n",
    "\n",
    "# load in model! (modeled after Angelina's Code :))\n",
    "\n",
    "def load_model(model_id = \"meta-llama/Llama-3.1-8B-Instruct\", device = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Load model and tokenizer, handling different architectures.\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Set padding token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    try:\n",
    "        # for encoder-only models (BERT, RoBERTa, etc.)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_id, \n",
    "            output_attentions=True,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "        )\n",
    "    except Exception:\n",
    "        try:\n",
    "            # For causal LM models (Llama, Qwen, GPT, etc.)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                output_attentions=True,\n",
    "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device == \"cuda\" else None\n",
    "            )\n",
    "        except Exception:\n",
    "            # for seq2seq models (BART, T5, etc.)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_id,\n",
    "                output_attentions=True,\n",
    "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "            )\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bd99f3-66e5-46d2-879f-5fedadcbf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_finite_value_comprehension(data):\n",
    "    \"\"\"Finds the highest value in a list, ignoring inf values, using list comprehension.\"\"\"\n",
    "    finite_values = [x for x in data if not math.isinf(x)]\n",
    "    return max(finite_values) if finite_values else -math.inf\n",
    "\n",
    "\n",
    "def get_second_value_ignoring_inf(data):\n",
    "    \"\"\"\n",
    "    Returns the second non-inf value in a list.\n",
    "\n",
    "    Args:\n",
    "      data: A list of numerical values.\n",
    "\n",
    "    Returns:\n",
    "      The second non-inf value in the list, or None if not found.\n",
    "    \"\"\"\n",
    "    non_inf_values = [x for x in data if not math.isinf(x)]\n",
    "    if len(non_inf_values) < 2:\n",
    "        return None\n",
    "    return non_inf_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b15fe32-8f03-49b0-8532-190a867cae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/klopez/tda_vs_linguistics/mitll2/lib/python3.12/site-packages/ripser/ripser.py:247: UserWarning: The input matrix is square, but the distance_matrix flag is off.  Did you mean to indicate that this was a distance matrix?\n",
      "  warnings.warn(\n",
      " 50%|█████     | 1/2 [00:00<00:00,  7.19it/s]/home/klopez/tda_vs_linguistics/mitll2/lib/python3.12/site-packages/ripser/ripser.py:247: UserWarning: The input matrix is square, but the distance_matrix flag is off.  Did you mean to indicate that this was a distance matrix?\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:00<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Num_0dim  Max_0dim  Max_0dim_Minus_Second  Mean_0dim  betti_curve_0  \\\n",
      "0         2  0.763900               0.684287   0.099306              8   \n",
      "1         2  1.240592               1.123554   0.120313              8   \n",
      "\n",
      "   persistence_entropy_0  Num_1dim  Max_1dim  Max_1dim_Minus_Second  \\\n",
      "0               1.690259         0         0                      0   \n",
      "1               1.194369         0         0                      0   \n",
      "\n",
      "   Mean_1dim  betti_curve_1  persistence_entropy_1  \n",
      "0          0              1                   -0.0  \n",
      "1          0              1                   -0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_attention(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    attention_matrices = torch.stack(outputs.attentions).mean(dim=0).squeeze().cpu().numpy()\n",
    "    return np.mean(attention_matrices, axis=0)  # Averaging across heads\n",
    "\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    # get the intial tokenization (layer 0 embeddings)\n",
    "    inputs = tokenizer(text=text, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # get first hidden state information\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    embeddings = outputs.hidden_states[0]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def build_graph_attention(attention_matrix, threshold=0.1):\n",
    "    graph = nx.Graph()\n",
    "    num_nodes = attention_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if attention_matrix[i, j] > threshold:\n",
    "                graph.add_edge(i, j, weight=attention_matrix[i, j])\n",
    "\n",
    "    if graph.number_of_nodes() == 0:\n",
    "        print(\"transposed matrix\")\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                if attention_matrix[j, i] > threshold:\n",
    "                    graph.add_edge(i, j, weight=attention_matrix[i, j])\n",
    "    \n",
    "    adjacency_matrix = nx.to_numpy_array(graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def build_graph_embeddings(embeddings):\n",
    "    # # standardize embeddings\n",
    "    if embeddings.ndim > 2:\n",
    "        embeddings = embeddings.reshape(-1, embeddings.shape[-1])\n",
    "    \n",
    "    # fit + transform PCA with 2 components\n",
    "    graph_embeddings = PCA(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "    return graph_embeddings\n",
    "\n",
    "def compute_tda_features(graph):\n",
    "    diagrams = ripser(graph, maxdim=1)['dgms']\n",
    "\n",
    "    h0 = diagrams[0]\n",
    "    h1 = diagrams[1] if len(diagrams) > 1 else np.array([])\n",
    "\n",
    "    num_h0 = np.count_nonzero(np.round(h0)) #count_nonzero(h0) #len(h0)\n",
    "    highest_h0 = find_highest_finite_value_comprehension(h0[:, 1] - h0[:, 0]) if num_h0 > 0 else 0\n",
    "    Second_highest_h0 = get_second_value_ignoring_inf(h0[:, 1] - h0[:, 0]) if num_h0 > 1 else 0\n",
    "    highest_minus_second_h0 = highest_h0 - Second_highest_h0 if num_h0 > 1 else 0\n",
    "\n",
    "    # Replace inf values with 0\n",
    "    h0[np.isinf(h0)] = 0\n",
    "    mean_h0 = np.mean(h0) if num_h0 > 0 else 0\n",
    "    # print(\"mean h0: \", mean_h0)\n",
    "\n",
    "\n",
    "    num_h1 = np.count_nonzero(np.round(h1))\n",
    "    highest_h1 = find_highest_finite_value_comprehension(h1[:, 1] - h1[:, 0]) if num_h1 > 0 else 0\n",
    "    second_highest_h1 = get_second_value_ignoring_inf(h1[:, 1] - h1[:, 0]) if num_h1 > 1 else 0\n",
    "    highest_minus_second_h1 = highest_h1 - second_highest_h1 if num_h1 > 1 else 0\n",
    "\n",
    "    # Replace inf values with 0\n",
    "    h1[np.isinf(h1)] = 0\n",
    "    mean_h1 = np.mean(h1) if num_h1 > 0 else 0\n",
    "    # print(\"mean h1: \", mean_h1)\n",
    "\n",
    "    h0_persistences = np.sort(h0[:, 1] - h0[:, 0]) if num_h0 > 1 else np.array([0])\n",
    "\n",
    "    h1_persistences = np.sort(h1[:, 1] - h1[:, 0]) if num_h1 > 1 else np.array([0])\n",
    "\n",
    "    # Additional TDA features for linguistic correlation\n",
    "    sum_persistence_0 = np.sum(h0_persistences) if len(h0_persistences) > 0 else 0\n",
    "    sum_persistence_1 = np.sum(h1_persistences) if len(h1_persistences) > 0 else 0\n",
    "    persistence_entropy_0 = -np.sum(h0_persistences * np.log(h0_persistences + 1e-10)) if len(h0_persistences) > 0 else 0\n",
    "    persistence_entropy_1 = -np.sum(h1_persistences * np.log(h1_persistences + 1e-10)) if len(h1_persistences) > 0 else 0\n",
    "    betti_curve_0 = len(h0_persistences)\n",
    "    betti_curve_1 = len(h1_persistences)\n",
    "\n",
    "\n",
    "    return [num_h0, highest_h0, highest_minus_second_h0, mean_h0, betti_curve_0, persistence_entropy_0,\n",
    "            num_h1, highest_h1, highest_minus_second_h1, mean_h1, betti_curve_1, persistence_entropy_1]\n",
    "    \n",
    "def process_texts(texts, lat_rep, model_id):\n",
    "    \"\"\"\n",
    "    input: - texts: array of strings\n",
    "           - lat_rep: string of which latent representation to use for PH analysis,\n",
    "             valid arguments are \"hs\" for hidden states or \"a\" for attention\n",
    "           - model_id: which model to use\n",
    "    \"\"\"\n",
    "    model, tokenizer = load_model(model_id)\n",
    "    \n",
    "    data = []\n",
    "    for text in tqdm(texts):\n",
    "        if lat_rep == \"hs\":\n",
    "            hidden_states = get_embeddings(text, model, tokenizer).cpu().detach().numpy()\n",
    "            graph = build_graph_embeddings(hidden_states)\n",
    "        elif lat_rep == \"a\":\n",
    "            attention_matrix = get_attention(text, model, tokenizer)\n",
    "            graph = build_graph_attention(attention_matrix)\n",
    "        else:\n",
    "            print(\"could not make a graph\")\n",
    "            return\n",
    "        tda_features = compute_tda_features(graph)\n",
    "        data.append(tda_features)\n",
    "\n",
    "    columns = [\"Num_0dim\", \"Max_0dim\", \"Max_0dim_Minus_Second\", \"Mean_0dim\", \"betti_curve_0\", \"persistence_entropy_0\",\n",
    "               \"Num_1dim\", \"Max_1dim\", \"Max_1dim_Minus_Second\", \"Mean_1dim\", \"betti_curve_1\", \"persistence_entropy_1\"]\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Example usage\n",
    "texts = [\"This is a test sentence.\", \"Another example of text processing.\"]\n",
    "df = process_texts(texts, \"a\", \"bert-base-uncased\")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f491d0-4d65-4e44-9a77-ae33406feda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset!!\n",
    "# datasets: BLIMP, windograd, GLUE, \n",
    "datasets = [\"~/tda_vs_linguistics/data/nyu_mll_blimp.csv\",\n",
    "            \"~/tda_vs_linguistics/data/nyu_winograd_wsc.csv\",\n",
    "            \"~/tda_vs_linguistics/data/nyu_mll_glue.csv\",\n",
    "            \"~/tda_vs_linguistics/data/nyu_mll_multi.csv\",\n",
    "            \"~/tda_vs_linguistics/data/galileo_snli.csv\",\n",
    "            \"~/tda_vs_linguistics/data/aps_superglue.csv\"]\n",
    "\n",
    "# automatically change name of data stored\n",
    "def heatmap_label(dataset):\n",
    "    dataset_name = \"\"\n",
    "    label_good = \"\"\n",
    "    label_bad = \"\"\n",
    "    title1 = \"\"\n",
    "    title2 = \"\"\n",
    "    if dataset == datasets[0]:\n",
    "        dataset_name = \"BLIMP\"\n",
    "        label_good = \"sentence_good\"\n",
    "        label_bad = \"sentence_bad\"\n",
    "        title1 = \"Gramatically Correct Sentences\"\n",
    "        title2 = \"Gramatically Incorrect Sentences\"\n",
    "    elif dataset == datasets[1]:\n",
    "        dataset_name = \"winograd\"\n",
    "        label_good = \"sentence1\"\n",
    "        label_bad = \"sentence2\"\n",
    "        title1 = \"Sentences Referencing Sooner Tokens\"\n",
    "        title2 = \"Sentences Referencing Later Tokens\"\n",
    "    elif dataset == datasets[2]:\n",
    "        dataset_name = \"GLUE\"\n",
    "        label_good = \"sentence1\"\n",
    "        label_bad = \"sentence2\"\n",
    "        title1 = \"GLUE: TDA vs. Linguistic Features\"\n",
    "        title2 = title1\n",
    "    elif dataset == datasets[3]:\n",
    "        dataset_name = \"MULTI NLI\"\n",
    "        label_good = \"premise\"\n",
    "        label_bad = \"hypothesis\"\n",
    "        title1 = \"MULTI NLI: TDA vs. Linguistic Features\"\n",
    "        title2 = title1\n",
    "    elif dataset == datasets[4]:\n",
    "        dataset_name = \"SNLI\"\n",
    "        label_good = \"premise\"\n",
    "        label_bad = \"hypothesis\"\n",
    "        title1 = \"SNLI: TDA vs. Linguistic Features\"\n",
    "        title2 = title1\n",
    "    elif dataset == datasets[5]:\n",
    "        dataset_name = \"SUPERGLUE\"\n",
    "        label_good = \"text\"\n",
    "        label_bad = label_good\n",
    "        title1 = \"SUPERGLUE: TDA vs. Linguistic Features\"\n",
    "        title2 = title1\n",
    "    return dataset_name, label_good, label_bad, title1, title2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to store relevant labels for model :)\n",
    "models = [\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "          \"Qwen/Qwen3-8B\",\n",
    "          \"Qwen/Qwen2.5-7B-Instruct-1M\",\n",
    "          \"openai-community/gpt2\",\n",
    "          \"distilbert/distilgpt2\",\n",
    "          \"FacebookAI/roberta-base\",\n",
    "          \"google-bert/bert-base-uncased\",\n",
    "          \"distilbert/distilbert-base-uncased\",\n",
    "          \"google/electra-small-discriminator\"]\n",
    "\n",
    "def which_model(model_id):\n",
    "    if model_id == models[0]:\n",
    "        model = \"Llama 3.1 8B Instruct\"\n",
    "        model_short = \"Llama\"\n",
    "    elif model_id == models[1]:\n",
    "        model = \"Qwen 3 8B\"\n",
    "        model_short = \"Qwen 3\"\n",
    "    elif model_id == models[2]:\n",
    "        model = \"Qwen 2.5 7B Instruct\"\n",
    "        model_short = \"Qwen 2.5\"\n",
    "    elif model_id == models[3]:\n",
    "        model = \"GPT2\"\n",
    "        model_short = \"gpt2\"\n",
    "    elif model_id == models[4]:\n",
    "        model = \"DistilGPT2\"\n",
    "        model_short = \"Distilgpt2\"\n",
    "    elif model_id == models[5]:\n",
    "        model = \"RoBERTa Base Model\"\n",
    "        model_short = \"RoBERTa\"\n",
    "    elif model_id == models[6]:\n",
    "        model = \"BERT Base (uncased)\"\n",
    "        model_short = \"BERT\"\n",
    "    elif model_id == models[7]:\n",
    "        model = \"DistilBERT Base (uncased)\"\n",
    "        model_short = \"DistilBERT\"\n",
    "    elif model_id == models[8]:\n",
    "        model = \"Electra Small Discriminator\"\n",
    "        model_short = \"ELECTRA\"\n",
    "    return model, model_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81a5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# make dataset for each model\n",
    "def make_csvs(model_id, dataset, num_sen = 500, bad = False):\n",
    "    # get labels for model\n",
    "    _, model_short = which_model(model_id)\n",
    "\n",
    "    dataset_name, label_good, label_bad, _, _ = heatmap_label(dataset)\n",
    "    data_sen_pairs = pd.read_csv(dataset)\n",
    "\n",
    "    # define \"good\" and \"bad\" sentences\n",
    "    sen_good = data_sen_pairs[label_good][0:num_sen]\n",
    "    good_tda = process_texts(sen_good, \"hs\", model_id)\n",
    "\n",
    "    good_tda_path = os.path.expanduser(f\"~/tda_vs_linguistics/{dataset_name}/{model_short}_{dataset_name}_good_tda.csv\")\n",
    "    good_tda.to_csv(good_tda_path, index=False)\n",
    "    print(f\"Added (good) texts from {dataset_name} for {model_short}!\")\n",
    "\n",
    "    if bad == True:\n",
    "        sen_bad = data_sen_pairs[label_bad][0:num_sen]\n",
    "        bad_tda = process_texts(sen_bad)\n",
    "\n",
    "        bad_tda_path = os.path.expanduser(f\"~/tda_vs_linguistics/{dataset_name}/{model_short}_{dataset_name}_bad_tda.csv\")\n",
    "        bad_tda.to_csv(bad_tda_path, index=False)\n",
    "        print(f\"Added (bad) texts from {dataset_name} for {model_short}!\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3251d22-cc4a-4c67-aad2-483dea953454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s]\n",
      " 35%|███▌      | 177/500 [00:05<00:09, 32.58it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m dataset = datasets[\u001b[32m3\u001b[39m]\n\u001b[32m      3\u001b[39m model_id = models[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmake_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mmake_csvs\u001b[39m\u001b[34m(model_id, dataset, num_sen, bad)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# define \"good\" and \"bad\" sentences\u001b[39;00m\n\u001b[32m     12\u001b[39m sen_good = data_sen_pairs[label_good][\u001b[32m0\u001b[39m:num_sen]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m good_tda = \u001b[43mprocess_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen_good\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m good_tda_path = os.path.expanduser(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m~/tda_vs_linguistics/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_short\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_good_tda.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m good_tda.to_csv(good_tda_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mprocess_texts\u001b[39m\u001b[34m(texts, lat_rep, model_id)\u001b[39m\n\u001b[32m    109\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcould not make a graph\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     tda_features = \u001b[43mcompute_tda_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     data.append(tda_features)\n\u001b[32m    114\u001b[39m columns = [\u001b[33m\"\u001b[39m\u001b[33mNum_0dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMax_0dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMax_0dim_Minus_Second\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMean_0dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbetti_curve_0\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpersistence_entropy_0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mNum_1dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMax_1dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMax_1dim_Minus_Second\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMean_1dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbetti_curve_1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpersistence_entropy_1\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mcompute_tda_features\u001b[39m\u001b[34m(graph)\u001b[39m\n\u001b[32m     66\u001b[39m highest_h1 = find_highest_finite_value_comprehension(h1[:, \u001b[32m1\u001b[39m] - h1[:, \u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m num_h1 > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     67\u001b[39m second_highest_h1 = get_second_value_ignoring_inf(h1[:, \u001b[32m1\u001b[39m] - h1[:, \u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m num_h1 > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m highest_minus_second_h1 = \u001b[43mhighest_h1\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_highest_h1\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m num_h1 > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Replace inf values with 0\u001b[39;00m\n\u001b[32m     71\u001b[39m h1[np.isinf(h1)] = \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# RUN IF TDA INFO FROM DATASET NOT STORED FOR MODEL\n",
    "dataset = datasets[3]\n",
    "model_id = models[0]\n",
    "make_csvs(model_id, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4b6d7-421e-4da4-9300-ab2130b5ea6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# good_tda = pd.read_csv(f\"~/tda_vs_linguistics/{dataset_name}/{model_short}_{dataset_name}_good_tda.csv\")\n",
    "# good_tda\n",
    "# bad_tda = pd.read_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_bad_tda.csv\")\n",
    "# bad_tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798a467-cc53-4417-9298-8eb09d05d226",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load your favourite spacy model (remember to install it first using e.g. `python -m spacy download en_core_web_sm`)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textdescriptives/all\")\n",
    "\n",
    "def get_desc(data):\n",
    "  doc = nlp(data.iloc[0])\n",
    "  df1 = td.extract_df(doc)\n",
    "\n",
    "  for text in tqdm(data.iloc[1:]):\n",
    "    text = nlp(text)\n",
    "    df2 = td.extract_df(text)\n",
    "    df1 = df1._append(df2)\n",
    "\n",
    "  return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32d530-8bad-4b3f-9bc2-411c0aa4071b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# good_feat = get_desc(sen_good)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# bad_feat = get_desc(sen_bad)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# good_feat.to_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_good_feat.csv\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# bad_feat.to_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_bad_feat.csv\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m good_feat = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m~/tda_vs_linguistics/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_good_feat.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m bad_feat = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m~/tda_vs_linguistics/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_bad_feat.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m good_feat\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "# good_feat = get_desc(sen_good)\n",
    "# bad_feat = get_desc(sen_bad)\n",
    "# good_feat.to_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_good_feat.csv\")\n",
    "# bad_feat.to_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_bad_feat.csv\")\n",
    "good_feat = pd.read_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_good_feat.csv\")\n",
    "bad_feat = pd.read_csv(f\"~/tda_vs_linguistics/{dataset_name}/{dataset_name}_bad_feat.csv\")\n",
    "good_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels_list = good_feat.columns.tolist()\n",
    "print(column_labels_list.index(\"doc_length\"))\n",
    "print(column_labels_list.index(\"pos_prop_ADP\"))\n",
    "print(column_labels_list.index(\"first_order_coherence\"))\n",
    "print(column_labels_list.index(\"second_order_coherence\"))\n",
    "print(column_labels_list.index(\"smog\"))\n",
    "print(column_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tda_list = good_tda.columns.tolist()\n",
    "print(tda_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122092bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## from Catherine's code ##\n",
    "# def compute_additional_features(texts):\n",
    "#     \"\"\"\n",
    "#     Compute additional linguistic features specifically suited for short sentences.\n",
    "#     Focus on syntactic and structural properties relevant to island effects.\n",
    "#     \"\"\"\n",
    "#     features_list = []\n",
    "    \n",
    "#     for text in tqdm(texts, desc=\"Computing additional features\"):\n",
    "#         doc = nlp(text)\n",
    "        \n",
    "#         features = {}\n",
    "        \n",
    "#         # ===== DEPENDENCY STRUCTURE FEATURES =====\n",
    "#         # Average dependency distance (how far apart are syntactic dependencies?)\n",
    "#         dep_distances = []\n",
    "#         for token in doc:\n",
    "#             if token.head != token:\n",
    "#                 dep_distances.append(abs(token.i - token.head.i))\n",
    "#         features['avg_dependency_distance'] = np.mean(dep_distances) if dep_distances else 0\n",
    "#         features['max_dependency_distance'] = max(dep_distances) if dep_distances else 0\n",
    "#         features['total_dependency_distance'] = sum(dep_distances) if dep_distances else 0\n",
    "        \n",
    "#         # Tree depth (how deep is the parse tree?)\n",
    "#         def get_tree_depth(token):\n",
    "#             if not list(token.children):\n",
    "#                 return 1\n",
    "#             return 1 + max(get_tree_depth(child) for child in token.children)\n",
    "        \n",
    "#         root_tokens = [token for token in doc if token.dep_ == 'ROOT']\n",
    "#         features['tree_depth'] = max([get_tree_depth(root) for root in root_tokens]) if root_tokens else 0\n",
    "        \n",
    "#         # Count of different dependency relations\n",
    "#         dep_types = Counter([token.dep_ for token in doc])\n",
    "#         features['n_dependency_types'] = len(dep_types)\n",
    "        \n",
    "#         # ===== WH-MOVEMENT FEATURES (Critical for island effects!) =====\n",
    "#         wh_words = ['who', 'what', 'where', 'when', 'why', 'how', 'which']\n",
    "#         features['has_wh_word'] = int(any(token.text.lower() in wh_words for token in doc))\n",
    "#         features['wh_word_position'] = next((token.i for token in doc if token.text.lower() in wh_words), -1)\n",
    "        \n",
    "#         # Distance from wh-word to main verb\n",
    "#         if features['wh_word_position'] >= 0:\n",
    "#             main_verbs = [token.i for token in doc if token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp', 'xcomp']]\n",
    "#             if main_verbs:\n",
    "#                 features['wh_to_verb_distance'] = min([abs(features['wh_word_position'] - v) for v in main_verbs])\n",
    "#             else:\n",
    "#                 features['wh_to_verb_distance'] = 0\n",
    "#         else:\n",
    "#             features['wh_to_verb_distance'] = 0\n",
    "        \n",
    "#         # ===== CLAUSE STRUCTURE =====\n",
    "#         # Count subordinate clauses (adjuncts!)\n",
    "#         subordinate_markers = ['after', 'before', 'while', 'although', 'because', 'if', 'when', 'since', 'until']\n",
    "#         features['n_subordinate_markers'] = sum(1 for token in doc if token.text.lower() in subordinate_markers)\n",
    "#         features['subordinate_marker_position'] = next((token.i for token in doc if token.text.lower() in subordinate_markers), -1)\n",
    "        \n",
    "#         # Count clauses (approximate by counting verbs)\n",
    "#         features['n_clauses'] = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "        \n",
    "#         # Subordinate clause depth (position relative to sentence length)\n",
    "#         if features['subordinate_marker_position'] >= 0:\n",
    "#             features['subordinate_relative_position'] = features['subordinate_marker_position'] / len(doc)\n",
    "#         else:\n",
    "#             features['subordinate_relative_position'] = 0\n",
    "        \n",
    "#         # ===== VERB FEATURES =====\n",
    "#         verbs = [token for token in doc if token.pos_ == 'VERB']\n",
    "#         features['n_verbs'] = len(verbs)\n",
    "        \n",
    "#         # Verb forms\n",
    "#         verb_forms = Counter([token.tag_ for token in verbs])\n",
    "#         features['n_gerunds'] = verb_forms.get('VBG', 0)  # -ing forms (e.g., \"shocking\")\n",
    "#         features['n_past_participles'] = verb_forms.get('VBN', 0)\n",
    "#         features['n_base_verbs'] = verb_forms.get('VB', 0)\n",
    "        \n",
    "#         # ===== PREPOSITION FEATURES (Critical for adjunct islands!) =====\n",
    "#         prepositions = [token for token in doc if token.pos_ == 'ADP']\n",
    "#         features['n_prepositions'] = len(prepositions)\n",
    "        \n",
    "#         # Distance between prepositions\n",
    "#         if len(prepositions) >= 2:\n",
    "#             prep_distances = [prepositions[i+1].i - prepositions[i].i for i in range(len(prepositions)-1)]\n",
    "#             features['avg_prep_distance'] = np.mean(prep_distances)\n",
    "#         else:\n",
    "#             features['avg_prep_distance'] = 0\n",
    "        \n",
    "#         # Position of first preposition\n",
    "#         features['first_prep_position'] = prepositions[0].i if prepositions else -1\n",
    "#         features['first_prep_relative_position'] = (prepositions[0].i / len(doc)) if prepositions else 0\n",
    "        \n",
    "#         # ===== NOUN PHRASE FEATURES =====\n",
    "#         noun_chunks = list(doc.noun_chunks)\n",
    "#         features['n_noun_phrases'] = len(noun_chunks)\n",
    "        \n",
    "#         if noun_chunks:\n",
    "#             features['avg_noun_phrase_length'] = np.mean([len(chunk) for chunk in noun_chunks])\n",
    "#             features['max_noun_phrase_length'] = max([len(chunk) for chunk in noun_chunks])\n",
    "#         else:\n",
    "#             features['avg_noun_phrase_length'] = 0\n",
    "#             features['max_noun_phrase_length'] = 0\n",
    "        \n",
    "#         # ===== WORD ORDER FEATURES =====\n",
    "#         # Check for inverted word order (common in questions)\n",
    "#         features['starts_with_wh'] = int(doc[0].text.lower() in wh_words if len(doc) > 0 else 0)\n",
    "#         features['starts_with_aux'] = int(doc[0].pos_ == 'AUX' if len(doc) > 0 else 0)\n",
    "        \n",
    "#         # Subject-verb distance\n",
    "#         subjects = [token for token in doc if 'subj' in token.dep_]\n",
    "#         verbs = [token for token in doc if token.pos_ == 'VERB']\n",
    "#         if subjects and verbs:\n",
    "#             subj_verb_distances = [abs(subj.i - verb.i) for subj in subjects for verb in verbs]\n",
    "#             features['min_subj_verb_distance'] = min(subj_verb_distances)\n",
    "#             features['avg_subj_verb_distance'] = np.mean(subj_verb_distances)\n",
    "#         else:\n",
    "#             features['min_subj_verb_distance'] = 0\n",
    "#             features['avg_subj_verb_distance'] = 0\n",
    "        \n",
    "#         # ===== EXTRACTION/MOVEMENT INDICATORS =====\n",
    "#         # Gap detection: look for traces of movement\n",
    "#         # In \"Who should Derek hug __?\", there's a gap after \"hug\"\n",
    "        \n",
    "#         # Count objects that could be extraction sites\n",
    "#         objects = [token for token in doc if token.dep_ in ['dobj', 'pobj', 'obj']]\n",
    "#         features['n_objects'] = len(objects)\n",
    "        \n",
    "#         # Distance from wh-word to potential gap\n",
    "#         if features['wh_word_position'] >= 0 and objects:\n",
    "#             gap_distances = [abs(features['wh_word_position'] - obj.i) for obj in objects]\n",
    "#             features['wh_to_gap_distance'] = min(gap_distances)\n",
    "#         else:\n",
    "#             features['wh_to_gap_distance'] = 0\n",
    "        \n",
    "#         # ===== PUNCTUATION FEATURES =====\n",
    "#         features['has_question_mark'] = int('?' in text)\n",
    "#         features['n_commas'] = text.count(',')\n",
    "        \n",
    "#         # ===== LENGTH FEATURES =====\n",
    "#         features['sentence_length'] = len(doc)\n",
    "#         features['n_words'] = len([token for token in doc if not token.is_punct])\n",
    "        \n",
    "#         # Average word length\n",
    "#         word_lengths = [len(token.text) for token in doc if not token.is_punct]\n",
    "#         features['avg_word_length'] = np.mean(word_lengths) if word_lengths else 0\n",
    "        \n",
    "#         # ===== SPECIFIC ISLAND VIOLATION INDICATORS =====\n",
    "#         # Check for the pattern: \"wh-word ... preposition ... verb-ing\"\n",
    "#         # This is the adjunct island structure\n",
    "#         if features['has_wh_word'] and features['n_subordinate_markers'] > 0:\n",
    "#             wh_pos = features['wh_word_position']\n",
    "#             sub_pos = features['subordinate_marker_position']\n",
    "#             gerunds = [token.i for token in doc if token.tag_ == 'VBG']\n",
    "            \n",
    "#             # Pattern: wh-word appears before subordinate clause with gerund\n",
    "#             if gerunds and wh_pos >= 0 and sub_pos >= 0:\n",
    "#                 features['island_pattern'] = int(wh_pos < sub_pos < min(gerunds))\n",
    "#             else:\n",
    "#                 features['island_pattern'] = 0\n",
    "#         else:\n",
    "#             features['island_pattern'] = 0\n",
    "        \n",
    "#         features_list.append(features)\n",
    "    \n",
    "#     return pd.DataFrame(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904eadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# additional_good = compute_additional_features(sen_good)\n",
    "# additional_bad = compute_additional_features(sen_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851bfce-2f6e-4031-a6b6-eecde2bff864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap labels\n",
    "linguistic_labels = [['entropy', 'perplexity', 'pos_prop_NOUN', 'pos_prop_ADJ', 'pos_prop_ADP',\n",
    "                              'first_order_coherence', 'flesch_reading_ease', 'flesch_kincaid_grade', 'smog',\n",
    "                              'n_stop_words', 'doc_length', 'n_unique_tokens'],\n",
    "                    ['avg_dependency_distance', 'max_dependency_distance', 'tree_depth',\n",
    "                              'wh_to_verb_distance', 'n_subordinate_markers', 'subordinate_relative_position',\n",
    "                              'n_gerunds', 'n_prepositions', 'first_prep_relative_position',\n",
    "                              'wh_to_gap_distance', 'island_pattern', 'avg_subj_verb_distance']]\n",
    "tda_labels = [[\"Num_0dim\", \"Max_0dim\", \"Mean_0dim\", \"betti_curve_0\", \"persistence_entropy_0\"], tda_list[:6]]\n",
    "\n",
    "linguistic_label = column_labels_list[3:] # column_labels_list[3:21] + column_labels_list[38:57] + column_labels_list[59:65] + column_labels_list[66:] # linguistic_labels[0]\n",
    "tda_label = tda_list # tda_labels[1]\n",
    "\n",
    "focus_good_feat = good_feat[linguistic_label]\n",
    "focus_bad_feat = bad_feat[linguistic_label]\n",
    "# focus_good_feat = additional_good[linguistic_label]\n",
    "# focus_bad_feat = additional_bad[linguistic_label]\n",
    "\n",
    "focus_tda_good = good_tda[tda_label]\n",
    "focus_tda_bad = bad_tda[tda_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8f198-e42a-4d37-a106-24565ca98725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to numpy\n",
    "tda_features = focus_tda_good.to_numpy()\n",
    "linguistic_features = focus_good_feat.to_numpy()\n",
    "\n",
    "# compute pairwise distance correlation matrix\n",
    "corr_matrix = np.zeros((tda_features.shape[1], linguistic_features.shape[1]))\n",
    "\n",
    "for i in range(tda_features.shape[1]):\n",
    "    for j in range(linguistic_features.shape[1]):\n",
    "        corr_matrix[i, j] = dcor.distance_correlation(tda_features[:, i], linguistic_features[:, j])\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(len(linguistic_label), len(tda_label)))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\",\n",
    "            xticklabels= linguistic_label,\n",
    "            yticklabels= tda_label\n",
    "            )\n",
    "\n",
    "\n",
    "plt.title(f\"{title1}\")\n",
    "\n",
    "# find highest correlated values\n",
    "red_threshold=0.5\n",
    "red_cells = [(tda_label[i], linguistic_label[j], corr_matrix[i, j]) \n",
    "             for i in range(corr_matrix.shape[0])\n",
    "             for j in range(corr_matrix.shape[1])\n",
    "             if (corr_matrix[i, j] > red_threshold)]\n",
    "\n",
    "print(\"Red cells (row, col, value):\")\n",
    "for cell in red_cells:\n",
    "    print(cell)\n",
    "\n",
    "attempt = 3\n",
    "path_good = os.path.expanduser(f\"~/tda_vs_linguistics/{dataset_name}/heatmaps/{dataset_name}_sen_good{attempt}.pdf\")\n",
    "\n",
    "plt.savefig(path_good)\n",
    "plt.show()\n",
    "\n",
    "#Distance Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca41a7-df94-4021-8908-38dcee3e9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to numpy\n",
    "tda_features2 = focus_tda_bad.to_numpy()\n",
    "linguistic_features2 = focus_bad_feat.to_numpy()\n",
    "\n",
    "# compute pairwise distance correlation matrix\n",
    "corr_matrix2 = np.zeros((tda_features2.shape[1], linguistic_features2.shape[1]))\n",
    "\n",
    "for i in range(tda_features2.shape[1]):\n",
    "    for j in range(linguistic_features2.shape[1]):\n",
    "        corr_matrix2[i, j] = dcor.distance_correlation(tda_features2[:, i], linguistic_features2[:, j])\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(len(linguistic_label), len(tda_label)))\n",
    "sns.heatmap(corr_matrix2, annot=True, cmap=\"coolwarm\",\n",
    "            xticklabels= linguistic_label,\n",
    "            yticklabels= tda_label\n",
    "            )\n",
    "\n",
    "# find highest correlated values\n",
    "red_threshold=0.5\n",
    "red_cells2 = [(tda_label[i], linguistic_label[j], corr_matrix[i, j]) \n",
    "             for i in range(corr_matrix2.shape[0])\n",
    "             for j in range(corr_matrix2.shape[1])\n",
    "             if (corr_matrix2[i, j] > red_threshold)]\n",
    "\n",
    "print(\"Red cells (row, col, value):\")\n",
    "for cell in red_cells2:\n",
    "    print(cell)\n",
    "\n",
    "path_bad = os.path.expanduser(f\"~/tda_vs_linguistics/{dataset_name}/heatmaps/{dataset_name}_sen_bad{attempt}.pdf\")\n",
    "\n",
    "plt.title(f\"{title2}\")\n",
    "plt.savefig(path_bad)\n",
    "plt.show()\n",
    "\n",
    "#Distance Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dee76b-1b8f-4d83-be12-c9cd1c40fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference matrix\n",
    "\n",
    "# compute pairwise distance correlation matrix\n",
    "corr_matrix3 = corr_matrix - corr_matrix2\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(len(linguistic_label), len(tda_label)))\n",
    "sns.heatmap(corr_matrix3, annot=True, cmap=\"coolwarm\",\n",
    "            xticklabels= linguistic_label,\n",
    "            yticklabels=tda_label\n",
    "            )\n",
    "\n",
    "# find highest correlated values\n",
    "red_threshold=0.5\n",
    "red_cells3 = [(tda_label[i], linguistic_label[j], corr_matrix3[i, j]) \n",
    "             for i in range(corr_matrix3.shape[0])\n",
    "             for j in range(corr_matrix3.shape[1])\n",
    "             if (corr_matrix3[i, j] > red_threshold)]\n",
    "\n",
    "print(\"Red cells (row, col, value):\")\n",
    "for cell in red_cells3:\n",
    "    print(cell)\n",
    "\n",
    "plt.title(f\"{dataset_name} Difference Matrix\")\n",
    "\n",
    "path_diff = os.path.expanduser(f\"~/tda_vs_linguistics/{dataset_name}/heatmaps/{dataset_name}_sen_diff{attempt}.pdf\")\n",
    "\n",
    "plt.savefig(path_diff)\n",
    "plt.show()\n",
    "\n",
    "#Distance Correlation Heatmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitll2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
